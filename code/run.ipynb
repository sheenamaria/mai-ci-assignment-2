{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "project_dir = Path.cwd().parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from data import Data\n",
    "from cnn import CNN\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "\n",
    "- Preprocess\n",
    "- Build splits (Training/Validation/Testing):\n",
    "  - 80/20/00 -> For the hyperparameter search\n",
    "  - 80/10/10\n",
    "  - 40/20/40\n",
    "  - 10/10/80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0 = Data('caltech101_silhouettes_28.mat', train_split=80, val_split=20, test_split=0)\n",
    "dataset1 = Data('caltech101_silhouettes_28.mat', train_split=80, val_split=10, test_split=10)\n",
    "dataset2 = Data('caltech101_silhouettes_28.mat', train_split=40, val_split=20, test_split=40)\n",
    "dataset3 = Data('caltech101_silhouettes_28.mat', train_split=10, val_split=10, test_split=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "Previous to the study of CNN configurations\n",
    "\n",
    "- OL Activation function\n",
    "- CFL Cost Function\n",
    "- Dense Layer Size\n",
    "- Learning rates"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import itertools\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "output_activations = ['softmax', 'sigmoid']\n",
    "cost_functions = ['categorical_crossentropy', 'mean_squared_error']\n",
    "learning_rates = [0.01, 0.1]\n",
    "dense_layer_sizes = [64, 128]\n",
    "max_epochs = 20\n",
    "\n",
    "def train_model_hyperparmeter_search(params):\n",
    "    try:\n",
    "        oa, cf, dls, lr = params\n",
    "        cnn = CNN(output_layer_activation=oa, filter_sizes=[64, 64], dense_layer_size=dls, hidden_layer_activation='tanh')\n",
    "        \n",
    "        print(f\"Training with parameters: OA={oa}, CF={cf}, DLS={dls}, LR={lr}\")\n",
    "        \n",
    "        history = cnn.fit(\n",
    "            dataset0,\n",
    "            cost_function=cf,\n",
    "            max_epochs=max_epochs,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "        \n",
    "        val_acc = history.history['val_accuracy'][-1]\n",
    "        return (oa, cf, dls, lr, val_acc, history)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing parameters {params}:\")\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "# Generate all parameter combinations\n",
    "param_combinations = list(itertools.product(\n",
    "    output_activations, \n",
    "    cost_functions, \n",
    "    dense_layer_sizes,\n",
    "    learning_rates\n",
    "))\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize\n",
    "hyperparameter_search_results = []\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Submit all tasks\n",
    "    futures = [executor.submit(train_model_hyperparmeter_search, params) for params in param_combinations]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            hyperparameter_search_results.append(result)\n",
    "\n",
    "# Sort and display results\n",
    "hyperparameter_search_results = sorted(hyperparameter_search_results, key=lambda x: x[-2], reverse=True)\n",
    "for result in hyperparameter_search_results:\n",
    "    print(f\"Params: {result[:-2]}, Validation Accuracy: {result[-2]:.4f}\")\n",
    "\n",
    "best_hyperparameters = hyperparameter_search_results[0][:-2]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list to a pickle file\n",
    "with open('hyperparameter_search_results.pkl', 'wb') as file:\n",
    "    pickle.dump(hyperparameter_search_results, file)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T16:51:15.243779Z",
     "start_time": "2024-12-16T16:51:14.034862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load hyperparameter search results\n",
    "with open('hyperparameter_search_results.pkl', 'rb') as file:\n",
    "    hyperparameter_search_results = pickle.load(file)\n",
    "\n",
    "# Define column names for the DataFrame\n",
    "columns = ['Output Activation', 'Cost Function', 'Dense Layer Size', 'Learning Rate', 'Validation Accuracy', 'History']\n",
    "\n",
    "# Convert the results into a pandas DataFrame\n",
    "results_df = pd.DataFrame(hyperparameter_search_results, columns=columns)\n",
    "\n",
    "# Drop the 'History' column as it's not needed for display\n",
    "results_df_no_history = results_df.drop(columns=['History'])\n",
    "\n",
    "# Find the row with the best accuracy\n",
    "best_row = results_df_no_history.loc[results_df_no_history['Validation Accuracy'].idxmax()]\n",
    "\n",
    "# Output directory\n",
    "output_dir = os.path.join(os.getcwd(), 'outputs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "png_file_path = os.path.join(output_dir, 'hyperparameter_search_results.png')\n",
    "\n",
    "# Add highlighting to the best row\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.axis('off')  # Turn off the axis\n",
    "\n",
    "# Add a table to the figure\n",
    "table_data = results_df_no_history.values.tolist()\n",
    "table_data.insert(0, results_df_no_history.columns.tolist())  # Add headers\n",
    "\n",
    "# Highlight the best row by changing its color\n",
    "cell_colors = [['white'] * len(results_df_no_history.columns) for _ in range(len(results_df_no_history) + 1)]\n",
    "best_row_index = results_df_no_history.index.get_loc(best_row.name) + 1  # Add 1 for headers\n",
    "cell_colors[best_row_index] = ['#FFFF99'] * len(results_df_no_history.columns)  # Yellow highlight\n",
    "\n",
    "# Create the table\n",
    "table = ax.table(cellText=table_data, \n",
    "                 cellColours=cell_colors,\n",
    "                 loc='center', \n",
    "                 cellLoc='center')\n",
    "\n",
    "# Style the table\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(8)\n",
    "table.scale(1.2, 1.2)\n",
    "\n",
    "# Save the table as a PNG file\n",
    "plt.savefig(png_file_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Table saved as PNG at: {png_file_path}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration for validation accuracy:\n",
      "Output Activation                       sigmoid\n",
      "Cost Function          categorical_crossentropy\n",
      "Dense Layer Size                             64\n",
      "Learning Rate                              0.01\n",
      "Validation Accuracy                    0.538329\n",
      "Name: 0, dtype: object\n",
      "Table saved as PNG at: /Users/sheena/Library/CloudStorage/Box-Box/Sheena./Master/semester-1/CI/Labs/LAB-EXERCISE1/mai-ci-assignment-2/code/outputs/hyperparameter_search_results.png\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Results\n",
    "\n",
    "Loop over configurations:\n",
    "1. Architecture:\n",
    "  - 1 Block: 128 Filter Size\n",
    "  - 3 Blocks: 32, 64 and 128 Filter Sizes\n",
    "2. Activations:\n",
    "  - Sigmoid\n",
    "  - ReLU\n",
    "3. Dataset Splits."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import itertools\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "oa = best_hyperparameters[0]\n",
    "cf =  best_hyperparameters[1]\n",
    "dls =  best_hyperparameters[2]\n",
    "lr =  best_hyperparameters[3]\n",
    "\n",
    "datasets = [dataset1, dataset2, dataset3]\n",
    "filter_sizes = [[128], [32, 64, 128]]\n",
    "hidden_activations = ['sigmoid', 'relu']\n",
    "\n",
    "def train_model_configuration_search(params):\n",
    "    try:\n",
    "        dataset, fs, nhl = params\n",
    "        cnn = CNN(output_layer_activation=oa, filter_sizes=fs, dense_layer_size=dls, hidden_layer_activation=nhl)\n",
    "        \n",
    "        splits = dataset.splits\n",
    "        \n",
    "        print(f\"Training with parameters: FS={fs}, NHL={nhl}, Splits={splits}\")\n",
    "        \n",
    "        history = cnn.fit(\n",
    "            dataset,\n",
    "            cost_function=cf,\n",
    "            max_epochs=max_epochs,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "        \n",
    "        val_acc = max(history.history['val_accuracy'])\n",
    "\n",
    "        test_acc = cnn.evaluate(dataset)\n",
    "\n",
    "        return (fs, nhl, splits, val_acc, test_acc[1], history)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing parameters {params} with dataset {splits}:\")\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "# Generate all parameter combinations\n",
    "param_combinations = list(itertools.product(\n",
    "    datasets,\n",
    "    filter_sizes,\n",
    "    hidden_activations\n",
    "))\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize\n",
    "configuration_search_results = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Submit all tasks\n",
    "    futures = [executor.submit(train_model_configuration_search, params) for params in param_combinations]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            configuration_search_results.append(result)\n",
    "\n",
    "# Sort and display results\n",
    "configuration_search_results = sorted(configuration_search_results, key=lambda x: x[-2], reverse=True)\n",
    "for result in configuration_search_results:\n",
    "    print(f\"Params: {result[:-3]}, Validation Accuracy: {result[-3]:.4f}, Testing Accuracy: {result[-2]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list to a pickle file\n",
    "with open('configuration_search_results.pkl', 'wb') as file:\n",
    "    pickle.dump(configuration_search_results, file)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T09:49:15.012956Z",
     "start_time": "2024-12-17T09:48:57.146170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the configuration search results\n",
    "with open('configuration_search_results.pkl', 'rb') as file:\n",
    "    configuration_search_results = pickle.load(file)\n",
    "\n",
    "# Find the architecture with the highest validation accuracy\n",
    "best_result = max(configuration_search_results, key=lambda x: x[-3])  # Validation Accuracy is at index -3\n",
    "best_history = best_result[-1]  # The history object is at the last position\n",
    "\n",
    "# Extract data from the history\n",
    "training_accuracy = best_history.history['accuracy']\n",
    "validation_accuracy = best_history.history['val_accuracy']\n",
    "training_loss = best_history.history['loss']\n",
    "validation_loss = best_history.history['val_loss']\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = os.path.join(os.getcwd(), 'outputs')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(training_accuracy, label='Training Accuracy')\n",
    "plt.plot(validation_accuracy, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(training_loss, label='Training Loss')\n",
    "plt.plot(validation_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot to the /outputs directory\n",
    "output_png_path = os.path.join(output_dir, 'best_configuration_training_validation.png')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_png_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Training and validation plots saved as PNG at: {output_png_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validation plots saved as PNG at: /Users/sheena/Library/CloudStorage/Box-Box/Sheena./Master/semester-1/CI/Labs/LAB-EXERCISE1/mai-ci-assignment-2/code/outputs/best_configuration_training_validation.png\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hyperparameter_search_results.pkl', 'rb') as file:\n",
    "    hyperparameter_search_results = pickle.load(file)\n",
    "\n",
    "with open('configuration_search_results.pkl', 'rb') as file:\n",
    "    configuration_search_results = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots and tables comparing the configurations.\n",
    "\n",
    "Tables with Validation Accuracies:\n",
    "- Hyperparameter search\n",
    "- Configuration search\n",
    "\n",
    "Plots:\n",
    "- Hyperparameter Heatmaps\n",
    "- Best run Train-Validation accuracy plot (need to save all of them during the search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "1) Description of the runs with the different configurations that you have performed. -> Sheena\n",
    "2) Explain how you have selected the rest of parameters. -> Sheena\n",
    "3) Those tables that you consider necessary to describe the results obtained for the different network configurations. Explain and reason the results presented in the tables.\n",
    "    - Tables -> Sheena\n",
    "    - Heatmaps -> Bruno\n",
    "    - Best run Train-Validation Accuracy plot -> Sheena\n",
    "4) Your own conclusions with respect the results obtained. -> Bruno"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
