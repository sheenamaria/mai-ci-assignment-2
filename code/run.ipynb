{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project directory to the Python path\n",
    "project_dir = Path.cwd().parent\n",
    "sys.path.append(str(project_dir))\n",
    "\n",
    "from data import Data\n",
    "from cnn import CNN\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "\n",
    "- Preprocess\n",
    "- Build splits (Training/Validation/Testing):\n",
    "  - 80/20/00 -> For the hyperparameter search\n",
    "  - 80/10/10\n",
    "  - 40/20/40\n",
    "  - 10/10/80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0 = Data('caltech101_silhouettes_28.mat', train_split=80, val_split=20, test_split=0)\n",
    "dataset1 = Data('caltech101_silhouettes_28.mat', train_split=80, val_split=10, test_split=10)\n",
    "dataset2 = Data('caltech101_silhouettes_28.mat', train_split=40, val_split=20, test_split=40)\n",
    "dataset3 = Data('caltech101_silhouettes_28.mat', train_split=10, val_split=10, test_split=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "Previous to the study of CNN configurations\n",
    "\n",
    "- OL Activation function\n",
    "- CFL Cost Function\n",
    "- Dense Layer Size\n",
    "- Learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: OA=softmax, CF=categorical_crossentropy, DLS=64, LR=0.01\n",
      "Training with parameters: OA=softmax, CF=categorical_crossentropy, DLS=64, LR=0.1\n",
      "Training with parameters: OA=softmax, CF=categorical_crossentropy, DLS=128, LR=0.01\n",
      "Training with parameters: OA=softmax, CF=categorical_crossentropy, DLS=128, LR=0.1\n",
      "Training with parameters: OA=softmax, CF=mean_squared_error, DLS=64, LR=0.01\n",
      "Training with parameters: OA=softmax, CF=mean_squared_error, DLS=64, LR=0.1\n",
      "Training with parameters: OA=softmax, CF=mean_squared_error, DLS=128, LR=0.01\n",
      "Training with parameters: OA=softmax, CF=mean_squared_error, DLS=128, LR=0.1\n",
      "Training with parameters: OA=sigmoid, CF=categorical_crossentropy, DLS=64, LR=0.01\n",
      "Training with parameters: OA=sigmoid, CF=categorical_crossentropy, DLS=64, LR=0.1\n",
      "Training with parameters: OA=sigmoid, CF=categorical_crossentropy, DLS=128, LR=0.01\n",
      "Training with parameters: OA=sigmoid, CF=categorical_crossentropy, DLS=128, LR=0.1\n",
      "Training with parameters: OA=sigmoid, CF=mean_squared_error, DLS=64, LR=0.01\n",
      "Training with parameters: OA=sigmoid, CF=mean_squared_error, DLS=64, LR=0.1\n",
      "Training with parameters: OA=sigmoid, CF=mean_squared_error, DLS=128, LR=0.01\n",
      "Training with parameters: OA=sigmoid, CF=mean_squared_error, DLS=128, LR=0.1\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "output_activations = ['softmax', 'sigmoid']\n",
    "cost_functions = ['categorical_crossentropy', 'mean_squared_error']\n",
    "learning_rates = [0.01, 0.1]\n",
    "dense_layer_sizes = [64, 128]\n",
    "max_epochs = 20\n",
    "\n",
    "def train_model_hyperparmeter_search(params):\n",
    "    try:\n",
    "        oa, cf, dls, lr = params\n",
    "        cnn = CNN(output_layer_activation=oa, filter_sizes=[64, 64], dense_layer_size=dls, hidden_layer_activation='tanh')\n",
    "        \n",
    "        print(f\"Training with parameters: OA={oa}, CF={cf}, DLS={dls}, LR={lr}\")\n",
    "        \n",
    "        history = cnn.fit(\n",
    "            dataset0,\n",
    "            cost_function=cf,\n",
    "            max_epochs=max_epochs,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "        \n",
    "        val_acc = history.history['val_accuracy'][-1]\n",
    "        return (oa, cf, dls, lr, val_acc, history)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing parameters {params}:\")\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "# Generate all parameter combinations\n",
    "param_combinations = list(itertools.product(\n",
    "    output_activations, \n",
    "    cost_functions, \n",
    "    dense_layer_sizes,\n",
    "    learning_rates\n",
    "))\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize\n",
    "hyperparameter_search_results = []\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Submit all tasks\n",
    "    futures = [executor.submit(train_model_hyperparmeter_search, params) for params in param_combinations]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            hyperparameter_search_results.append(result)\n",
    "\n",
    "# Sort and display results\n",
    "hyperparameter_search_results = sorted(hyperparameter_search_results, key=lambda x: x[-2], reverse=True)\n",
    "for result in hyperparameter_search_results:\n",
    "    print(f\"Params: {result[:-2]}, Validation Accuracy: {result[-2]:.4f}\")\n",
    "\n",
    "best_hyperparameters = hyperparameter_search_results[0][:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list to a pickle file\n",
    "with open('hyperparameter_search_results.pkl', 'wb') as file:\n",
    "    pickle.dump(hyperparameter_search_results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Results\n",
    "\n",
    "Loop over configurations:\n",
    "1. Architecture:\n",
    "  - 1 Block: 128 Filter Size\n",
    "  - 3 Blocks: 32, 64 and 128 Filter Sizes\n",
    "2. Activations:\n",
    "  - Sigmoid\n",
    "  - ReLU\n",
    "3. Dataset Splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: FS=[128], NHL=sigmoid, Splits=(80, 10, 10)\n",
      "Training with parameters: FS=[128], NHL=relu, Splits=(80, 10, 10)\n",
      "Training with parameters: FS=[32, 64, 128], NHL=sigmoid, Splits=(80, 10, 10)\n",
      "Training with parameters: FS=[32, 64, 128], NHL=relu, Splits=(80, 10, 10)\n",
      "Training with parameters: FS=[128], NHL=sigmoid, Splits=(40, 20, 40)\n",
      "Training with parameters: FS=[128], NHL=relu, Splits=(40, 20, 40)\n",
      "Training with parameters: FS=[32, 64, 128], NHL=sigmoid, Splits=(40, 20, 40)\n",
      "Training with parameters: FS=[32, 64, 128], NHL=relu, Splits=(40, 20, 40)\n",
      "Training with parameters: FS=[128], NHL=sigmoid, Splits=(10, 10, 80)\n",
      "Training with parameters: FS=[128], NHL=relu, Splits=(10, 10, 80)\n",
      "Training with parameters: FS=[32, 64, 128], NHL=sigmoid, Splits=(10, 10, 80)\n",
      "Training with parameters: FS=[32, 64, 128], NHL=relu, Splits=(10, 10, 80)\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.5014 - loss: 4.9274\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.0971 - loss: 4.4401\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.4297 - loss: 8.8901\n",
      "\u001b[1m217/217\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.0859 - loss: 4.2651\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.5598 - loss: 6.0035\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0986 - loss: 4.3239\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.4640 - loss: 7.6363\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0986 - loss: 4.1936\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5605 - loss: 6.1884\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5148 - loss: 4.8934\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0757 - loss: 4.4072\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0757 - loss: 4.2206 \n",
      "Params: ([128], 'relu', (80, 10, 10), 0.5997692942619324), Validation Accuracy: 0.5998, Testing Accuracy: 0.5876\n",
      "Params: ([128], 'relu', (40, 20, 40), 0.5813148617744446), Validation Accuracy: 0.5813, Testing Accuracy: 0.5653\n",
      "Params: ([32, 64, 128], 'relu', (80, 10, 10), 0.5732410550117493), Validation Accuracy: 0.5732, Testing Accuracy: 0.5357\n",
      "Params: ([128], 'relu', (10, 10, 80), 0.47058823704719543), Validation Accuracy: 0.4706, Testing Accuracy: 0.5040\n",
      "Params: ([32, 64, 128], 'relu', (40, 20, 40), 0.5092272162437439), Validation Accuracy: 0.5092, Testing Accuracy: 0.4774\n",
      "Params: ([32, 64, 128], 'relu', (10, 10, 80), 0.423298716545105), Validation Accuracy: 0.4233, Testing Accuracy: 0.4303\n",
      "Params: ([128], 'sigmoid', (40, 20, 40), 0.10322953015565872), Validation Accuracy: 0.1032, Testing Accuracy: 0.1015\n",
      "Params: ([32, 64, 128], 'sigmoid', (40, 20, 40), 0.10322953015565872), Validation Accuracy: 0.1032, Testing Accuracy: 0.1015\n",
      "Params: ([128], 'sigmoid', (10, 10, 80), 0.09342560917139053), Validation Accuracy: 0.0934, Testing Accuracy: 0.0941\n",
      "Params: ([32, 64, 128], 'sigmoid', (10, 10, 80), 0.09342560917139053), Validation Accuracy: 0.0934, Testing Accuracy: 0.0923\n",
      "Params: ([128], 'sigmoid', (80, 10, 10), 0.09573241323232651), Validation Accuracy: 0.0957, Testing Accuracy: 0.0829\n",
      "Params: ([32, 64, 128], 'sigmoid', (80, 10, 10), 0.09573241323232651), Validation Accuracy: 0.0957, Testing Accuracy: 0.0829\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "oa = best_hyperparameters[0]\n",
    "cf =  best_hyperparameters[1]\n",
    "dls =  best_hyperparameters[2]\n",
    "lr =  best_hyperparameters[3]\n",
    "\n",
    "datasets = [dataset1, dataset2, dataset3]\n",
    "filter_sizes = [[128], [32, 64, 128]]\n",
    "hidden_activations = ['sigmoid', 'relu']\n",
    "\n",
    "def train_model_configuration_search(params):\n",
    "    try:\n",
    "        dataset, fs, nhl = params\n",
    "        cnn = CNN(output_layer_activation=oa, filter_sizes=fs, dense_layer_size=dls, hidden_layer_activation=nhl)\n",
    "        \n",
    "        splits = dataset.splits\n",
    "        \n",
    "        print(f\"Training with parameters: FS={fs}, NHL={nhl}, Splits={splits}\")\n",
    "        \n",
    "        history = cnn.fit(\n",
    "            dataset,\n",
    "            cost_function=cf,\n",
    "            max_epochs=max_epochs,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "        \n",
    "        val_acc = max(history.history['val_accuracy'])\n",
    "\n",
    "        test_acc = cnn.evaluate(dataset)\n",
    "\n",
    "        return (fs, nhl, splits, val_acc, test_acc[1], history)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing parameters {params} with dataset {splits}:\")\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "# Generate all parameter combinations\n",
    "param_combinations = list(itertools.product(\n",
    "    datasets,\n",
    "    filter_sizes,\n",
    "    hidden_activations\n",
    "))\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize\n",
    "configuration_search_results = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Submit all tasks\n",
    "    futures = [executor.submit(train_model_configuration_search, params) for params in param_combinations]\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            configuration_search_results.append(result)\n",
    "\n",
    "# Sort and display results\n",
    "configuration_search_results = sorted(configuration_search_results, key=lambda x: x[-2], reverse=True)\n",
    "for result in configuration_search_results:\n",
    "    print(f\"Params: {result[:-3]}, Validation Accuracy: {result[-3]:.4f}, Testing Accuracy: {result[-2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list to a pickle file\n",
    "with open('configuration_search_results.pkl', 'wb') as file:\n",
    "    pickle.dump(configuration_search_results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hyperparameter_search_results.pkl', 'rb') as file:\n",
    "    hyperparameter_search_results = pickle.load(file)\n",
    "\n",
    "with open('configuration_search_results.pkl', 'rb') as file:\n",
    "    configuration_search_results = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots and tables comparing the configurations.\n",
    "\n",
    "Tables with Validation Accuracies:\n",
    "- Hyperparameter search\n",
    "- Configuration search\n",
    "\n",
    "Plots:\n",
    "- Hyperparameter Heatmaps\n",
    "- Best run Train-Validation accuracy plot (need to save all of them during the search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "1) Description of the runs with the different configurations that you have performed. -> Sheena\n",
    "2) Explain how you have selected the rest of parameters. -> Sheena\n",
    "3) Those tables that you consider necessary to describe the results obtained for the different network configurations. Explain and reason the results presented in the tables.\n",
    "    - Tables -> Sheena\n",
    "    - Heatmaps -> Bruno\n",
    "    - Best run Train-Validation Accuracy plot -> Sheena\n",
    "4) Your own conclusions with respect the results obtained. -> Bruno"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
